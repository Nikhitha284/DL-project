{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b25f1d3a-e721-48a0-b017-684abc32124f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "0/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "1/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "2/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "3/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "4/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "5/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "6/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "7/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "8/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "9/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "10/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "11/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "12/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "13/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "14/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "15/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "16/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "17/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "18/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "19/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "20/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "21/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "22/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "23/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "24/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "25/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "26/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "27/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "28/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "29/30\n",
      "[ 64.  55.  34. 193.  54. 117.  60.  57.  79. 184.  33.  58.  78. 122.\n",
      "   4.   9.  42.  36. 103.  76.  35.  18. 144.  48.  11.  25.  97.  66.\n",
      " 199. 129.]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Nadam\n",
    "import tensorflow as tf\n",
    "from scipy.spatial.distance import euclidean, correlation\n",
    "from scipy.stats import entropy\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "\n",
    "\n",
    "### #0. hyper-parameters & data\n",
    "nb_bands = 200\n",
    "nb_exp_bands = 30\n",
    "state_size = nb_bands\n",
    "action_size = nb_bands\n",
    "learning_rate = 0.0001\n",
    "\n",
    "data = sio.loadmat('data4drl/data_indian_pines_drl.mat')\n",
    "x = np.float32(data['x'])\n",
    "\n",
    "IEs = np.zeros((nb_bands,))\n",
    "for i in range(nb_bands):\n",
    "    IEs[i] = entropy(x[:, i])\n",
    "IEs = (IEs-np.min(IEs))/(np.max(IEs)-np.min(IEs))\n",
    "\n",
    "### step #1. defining the agent (dqn)\n",
    "class dqn4hsi(object):\n",
    "    def __init__(self, state_size, action_size, learning_rate):\n",
    "    \tself.state_size = state_size\n",
    "    \tself.action_size = action_size\n",
    "    \tself.memory = deque(maxlen = 50000)\n",
    "    \tself.gamma = 0.99\n",
    "    \tself.epsilon = 1.0\n",
    "    \tself.epsilon_min = 0.01\n",
    "    \tself.epsilon_decay = 0.995\n",
    "    \tself.learning_rate = learning_rate\n",
    "    \tself.train_batch = 100\n",
    "    \tself._model = self._createModel()\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "    \treturn self._model\n",
    "\n",
    "    def _createModel(self):\n",
    "    \tmodel = Sequential()\n",
    "    \tmodel.add(Dense(2*self.state_size, input_shape = (self.state_size,), activation = 'relu'))\n",
    "    \tmodel.add(Dense(2*self.state_size, activation = 'relu'))\n",
    "    \tmodel.add(Dense(self.action_size, activation = 'linear'))\n",
    "    \tmodel.compile(loss = 'mse', optimizer = Nadam(learning_rate = self.learning_rate))\n",
    "    \treturn model\n",
    "\t\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "    \tself.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def predictAction(self, state):\n",
    "    \treturn self.model.predict(state)\n",
    "\n",
    "    def act(self, state):\n",
    "    \tstate = state.reshape((self.state_size,))\n",
    "    \tif random.random()<self.epsilon:\n",
    "    \t\taction_set = np.squeeze(np.argwhere(state==0))\n",
    "    \t\treturn random.sample(action_set, 1)[0]\n",
    "    \telse:\n",
    "    \t\tinvalid_set = np.squeeze(np.argwhere(state>0))\n",
    "    \t\tstate = state.reshape((1, self.state_size))\n",
    "    \t\tprob = self.predictAction(state)[0]\n",
    "    \t\tprob[invalid_set] = -99999\n",
    "    \t\treturn np.argmax(prob)\n",
    "\n",
    "    def replay(self):\n",
    "    \tif len(self.memory)>=self.train_batch:\n",
    "    \t\tminibatch = random.sample(self.memory, self.train_batch)\n",
    "    \t\tstate_batch = np.zeros((self.train_batch, self.state_size))\n",
    "    \t\ttarget_batch = np.zeros((self.train_batch, self.action_size))\n",
    "    \t\tfor i, (state, action, reward, next_state, done) in enumerate(minibatch):\n",
    "    \t\t\tstate_batch[i, :] = state\n",
    "    \t\t\ttarget_batch[i, :] = self.predictAction(state)\n",
    "    \tif done:\n",
    "    \t\ttarget_batch[i, action] = reward\n",
    "    \telse:\n",
    "    \t\tnext_prob = self.predictAction(next_state)[0]\n",
    "    \t\tnext_state = next_state.reshape((self.state_size,))\n",
    "    \t\tinvalid_set = np.squeeze(np.argwhere(next_state>0))\n",
    "    \t\tnext_prob[invalid_set] = -99999\n",
    "    \t\ttarget_batch[i, action] = reward+self.gamma*np.argmax(next_prob)\n",
    "    \thistory = self.model.fit(state_batch, target_batch, epochs = 1, verbose = 0)\n",
    "    \tloss = history.history['loss'][0]\n",
    "    \t\n",
    "    \tif self.epsilon>self.epsilon_min:\n",
    "    \t\tself.epsilon*=self.epsilon_decay\n",
    "    \t\treturn loss\n",
    "    \telse:\n",
    "    \t\tprint('just a moment...')\n",
    "    def loadWeights(self, name):\n",
    "        self.model.load_weights(name)\n",
    "    def saveWeights(self, name):\n",
    "        self.model.save_weights(name)\n",
    "\t\n",
    "\n",
    "### step #2. inference\n",
    "agent = dqn4hsi(state_size, action_size, learning_rate)\n",
    "agent.loadWeights('models/qnet_indian_pines_30_bands.h5')\n",
    "\n",
    "selected_bands = np.zeros((nb_exp_bands,))\n",
    "state = np.float32(np.zeros((1, state_size)))\n",
    "for t in range(nb_exp_bands):\n",
    "    state = state.reshape((state_size,))\n",
    "    invalid_set = np.squeeze(np.argwhere(state>0))\n",
    "    state = state.reshape((1, state_size))\n",
    "    pred = agent.predictAction(state)[0]\n",
    "    pred[invalid_set] = -99999\n",
    "    action = np.argmax(pred)\n",
    "    selected_bands[t] = action\n",
    "    state[0, action] = 1\n",
    "\n",
    "    print(\"{}/{}\".format(t, nb_exp_bands))\n",
    "\n",
    "print (selected_bands)\n",
    "\n",
    "sio.savemat('results/drl_30_bands_indian_pines.mat', {'selected_bands': selected_bands})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012349c5-8978-48d7-ac45-cacc85668d72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
