{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7b3d021-d18a-4996-b0dd-67dd8e10d7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x: (21025, 200)\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "data = sio.loadmat('data_indian_pines_drl.mat')\n",
    "x = np.float32(data['x'])\n",
    "\n",
    "print(\"Shape of x:\", x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c51cd95-5c11-4759-8e68-f55a50a91c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-08 18:38:53.175301: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-08 18:38:53.187578: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-08 18:38:53.293320: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-08 18:38:53.385022: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-08 18:38:53.457801: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-08 18:38:53.484351: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-08 18:38:53.667959: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-08 18:38:54.951809: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 26ms/step - accuracy: 0.0993 - loss: 2.3029 - val_accuracy: 0.0994 - val_loss: 2.3047\n",
      "Epoch 2/10\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 29ms/step - accuracy: 0.1102 - loss: 2.3028 - val_accuracy: 0.0951 - val_loss: 2.3033\n",
      "Epoch 3/10\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 28ms/step - accuracy: 0.1056 - loss: 2.3024 - val_accuracy: 0.0951 - val_loss: 2.3034\n",
      "Epoch 4/10\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 28ms/step - accuracy: 0.1096 - loss: 2.3021 - val_accuracy: 0.0951 - val_loss: 2.3032\n",
      "Epoch 5/10\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 31ms/step - accuracy: 0.1056 - loss: 2.3024 - val_accuracy: 0.0951 - val_loss: 2.3033\n",
      "Epoch 6/10\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 31ms/step - accuracy: 0.1079 - loss: 2.3025 - val_accuracy: 0.0951 - val_loss: 2.3034\n",
      "Epoch 7/10\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 33ms/step - accuracy: 0.1036 - loss: 2.3026 - val_accuracy: 0.0951 - val_loss: 2.3034\n",
      "Epoch 8/10\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 31ms/step - accuracy: 0.1044 - loss: 2.3028 - val_accuracy: 0.0951 - val_loss: 2.3036\n",
      "Epoch 9/10\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 33ms/step - accuracy: 0.1075 - loss: 2.3021 - val_accuracy: 0.0951 - val_loss: 2.3035\n",
      "Epoch 10/10\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 31ms/step - accuracy: 0.1056 - loss: 2.3024 - val_accuracy: 0.0951 - val_loss: 2.3034\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, GlobalAveragePooling2D, GlobalMaxPooling2D, Concatenate, Multiply, Reshape, Lambda\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "\n",
    "# Load and preprocess data\n",
    "import scipy.io as sio\n",
    "data = sio.loadmat('data_indian_pines_drl.mat')\n",
    "x = np.float32(data['x'])\n",
    "\n",
    "# Reshape x to fit into the CNN model\n",
    "x_reshaped = x.reshape((21025, 200, 1, 1))  # Adjust if needed\n",
    "scaler = MinMaxScaler()\n",
    "x_normalized = scaler.fit_transform(x.reshape(-1, x.shape[-1])).reshape(x.shape)\n",
    "\n",
    "# Define the attention-based CNN model\n",
    "def build_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Define the CNN layers\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = MaxPooling2D(pool_size=(1, 1))(x)  # Pooling with (1, 1) to avoid reducing dimensions too much\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D(pool_size=(1, 1))(x)  # Pooling with (1, 1)\n",
    "    \n",
    "    # Attention module\n",
    "    def attention_module(x):\n",
    "        # Channel attention\n",
    "        avg_pool = GlobalAveragePooling2D()(x)\n",
    "        max_pool = GlobalMaxPooling2D()(x)\n",
    "        avg_pool = Reshape((1, 1, x.shape[-1]))(avg_pool)\n",
    "        max_pool = Reshape((1, 1, x.shape[-1]))(max_pool)\n",
    "        concat = Concatenate(axis=-1)([avg_pool, max_pool])\n",
    "        concat = Conv2D(x.shape[-1], (1, 1), activation='sigmoid')(concat)\n",
    "        channel_att = Multiply()([x, concat])\n",
    "        \n",
    "        # Spatial attention\n",
    "        avg_pool = Lambda(lambda x: tf.reduce_mean(x, axis=-1, keepdims=True))(channel_att)\n",
    "        max_pool = Lambda(lambda x: tf.reduce_max(x, axis=-1, keepdims=True))(channel_att)\n",
    "        concat = Concatenate(axis=-1)([avg_pool, max_pool])\n",
    "        concat = Conv2D(1, (7, 7), activation='sigmoid', padding='same')(concat)\n",
    "        spatial_att = Multiply()([x, concat])\n",
    "        \n",
    "        return spatial_att\n",
    "    \n",
    "    x = attention_module(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    outputs = Dense(10, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Prepare the data for training\n",
    "y = np.random.randint(0, 10, size=(x_reshaped.shape[0],))  # Example labels, replace with actual labels\n",
    "\n",
    "# Split into training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(x_reshaped, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build and train the model\n",
    "input_shape = (x_reshaped.shape[1], x_reshaped.shape[2], x_reshaped.shape[3])\n",
    "model = build_model(input_shape)\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32)\n",
    "\n",
    "# Extract attention weights\n",
    "attention_layer = model.get_layer(index=3)  # Adjust index as needed\n",
    "attention_model = Model(inputs=model.input, outputs=attention_layer.output)\n",
    "attention_heatmaps = attention_model.predict(X_val)\n",
    "\n",
    "# Convert attention heatmaps to numpy for further analysis\n",
    "attention_scores = np.mean(attention_heatmaps, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "186855a6-96b0-4f3a-925b-86b7321cac16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harsha/.local/lib/python3.10/site-packages/sklearn/covariance/_robust_covariance.py:747: UserWarning: The covariance matrix associated to your dataset is not full rank\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of important bands: 40\n",
      "Indices of important bands: [  0   1   2   3   4   9  10  13  14  18  19  20  38  40  41  42  43  44\n",
      "  46  47  49  51  52  53  54  56  58  59  60  61  62  63  65  66  73 103\n",
      " 104 106 144 199]\n"
     ]
    }
   ],
   "source": [
    "attention_scores_flattened = attention_scores.reshape(-1, attention_scores.shape[-1])\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "attention_scores_flattened = scaler.fit_transform(attention_scores_flattened)\n",
    "\n",
    "# Debugging: Check if there are any NaNs or infinities in the data\n",
    "if np.any(np.isnan(attention_scores_flattened)) or np.any(np.isinf(attention_scores_flattened)):\n",
    "    raise ValueError(\"Data contains NaNs or infinities. Check the data preprocessing.\")\n",
    "\n",
    "# Initialize EllipticEnvelope with a larger support_fraction if needed\n",
    "ee = EllipticEnvelope(contamination=0.15, support_fraction=0.8)  # Adjust support_fraction as needed\n",
    "\n",
    "try:\n",
    "    # Fit the model\n",
    "    ee.fit(attention_scores_flattened)\n",
    "    \n",
    "    # Predict anomalies\n",
    "    important_bands = ee.predict(attention_scores_flattened)\n",
    "    \n",
    "    # Get indices where important_bands == -1\n",
    "    important_band_indices = np.where(important_bands == -1)[0]\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Number of important bands:\", len(important_band_indices))\n",
    "    print(\"Indices of important bands:\", important_band_indices)\n",
    "    \n",
    "except ValueError as e:\n",
    "    print(\"Error during fitting or prediction:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8b5372a7-a2f3-4706-8c71-8ae700a27408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of important bands: 36\n",
      "Important Bands:\n",
      "[-1 -1 -1 -1 -1  1  1  1  1 -1 -1  1  1 -1 -1  1  1 -1 -1 -1 -1 -1 -1 -1\n",
      "  1  1  1  1  1  1 -1  1  1  1  1  1  1  1 -1  1 -1  1  1  1 -1  1  1  1\n",
      "  1 -1  1  1  1  1  1  1  1  1 -1 -1 -1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1 -1 -1 -1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1\n",
      " -1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1\n",
      "  1  1  1 -1  1 -1 -1 -1]\n",
      "Indices of important bands: [  0   1   2   3   4   9  10  13  14  17  18  19  20  21  22  23  30  38\n",
      "  40  44  49  58  59  60 102 103 104 106 143 144 150 184 195 197 198 199]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Use Isolation Forest for anomaly detection\n",
    "iso_forest = IsolationForest(contamination=0.18)\n",
    "important_bands = iso_forest.fit_predict(attention_scores_flattened)\n",
    "\n",
    "print(\"Number of important bands:\", np.sum(important_bands == -1))  # -1 indicates anomalies\n",
    "print(\"Important Bands:\")\n",
    "print(important_bands)\n",
    "# Assuming `important_bands` contains the classification results from Isolation Forest\n",
    "# Get indices where important_bands == -1\n",
    "important_band_indices = np.where(important_bands == -1)[0]\n",
    "\n",
    "print(\"Indices of important bands:\", important_band_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "14ef5628-f6f2-4e10-83f0-5f449e21ac76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of important bands: 10\n",
      "Important Bands: [ 1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1\n",
      " -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1 -1  1  1  1  1  1  1 -1  1  1 -1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1 -1  1  1\n",
      "  1  1  1  1 -1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1]\n",
      "Indices of important bands: [  8  39  48 102 109 112 162 165 172 174]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# Apply LOF for anomaly detection\n",
    "lof = LocalOutlierFactor(contamination=0.05)\n",
    "important_bands = lof.fit_predict(attention_scores_flattened)\n",
    "\n",
    "# Print results\n",
    "num_important_bands = np.sum(important_bands == -1)\n",
    "print(\"Number of important bands:\", num_important_bands)\n",
    "print(\"Important Bands:\", important_bands)\n",
    "# Assuming `important_bands` contains the classification results from Isolation Forest\n",
    "# Get indices where important_bands == -1\n",
    "important_band_indices = np.where(important_bands == -1)[0]\n",
    "\n",
    "print(\"Indices of important bands:\", important_band_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fc8c21-b8f2-4b79-8655-18b04d071f39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
