{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7b3d021-d18a-4996-b0dd-67dd8e10d7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x: (21025, 200)\n"
     ]
    }
   ],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "data = sio.loadmat('data_indian_pines_drl.mat')\n",
    "x = np.float32(data['x'])\n",
    "\n",
    "print(\"Shape of x:\", x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c51cd95-5c11-4759-8e68-f55a50a91c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.1004 - loss: 2.3036 - val_accuracy: 0.0956 - val_loss: 2.3026\n",
      "Epoch 2/10\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 34ms/step - accuracy: 0.0968 - loss: 2.3027 - val_accuracy: 0.0956 - val_loss: 2.3028\n",
      "Epoch 3/10\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 30ms/step - accuracy: 0.1020 - loss: 2.3024 - val_accuracy: 0.0956 - val_loss: 2.3027\n",
      "Epoch 4/10\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 34ms/step - accuracy: 0.1051 - loss: 2.3025 - val_accuracy: 0.0982 - val_loss: 2.3027\n",
      "Epoch 5/10\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 39ms/step - accuracy: 0.1046 - loss: 2.3025 - val_accuracy: 0.1020 - val_loss: 2.3027\n",
      "Epoch 6/10\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 38ms/step - accuracy: 0.0973 - loss: 2.3027 - val_accuracy: 0.0956 - val_loss: 2.3028\n",
      "Epoch 7/10\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 32ms/step - accuracy: 0.0964 - loss: 2.3026 - val_accuracy: 0.0956 - val_loss: 2.3028\n",
      "Epoch 8/10\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 36ms/step - accuracy: 0.1058 - loss: 2.3026 - val_accuracy: 0.0956 - val_loss: 2.3028\n",
      "Epoch 9/10\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 32ms/step - accuracy: 0.1054 - loss: 2.3025 - val_accuracy: 0.0956 - val_loss: 2.3029\n",
      "Epoch 10/10\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 30ms/step - accuracy: 0.1017 - loss: 2.3024 - val_accuracy: 0.0956 - val_loss: 2.3028\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "Error with EllipticEnvelope: Found array with dim 3. MinCovDet expected <= 2.\n",
      "Number of important bands: [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "Attention Scores:\n",
      "[[[0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]]]\n",
      "Important Bands:\n",
      "[[[0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, GlobalAveragePooling2D, GlobalMaxPooling2D, Concatenate, Multiply, Reshape, Lambda\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "\n",
    "# Load and preprocess data\n",
    "import scipy.io as sio\n",
    "data = sio.loadmat('data_indian_pines_drl.mat')\n",
    "x = np.float32(data['x'])\n",
    "\n",
    "# Reshape x to fit into the CNN model\n",
    "x_reshaped = x.reshape((21025, 200, 1, 1))  # Adjust if needed\n",
    "scaler = MinMaxScaler()\n",
    "x_normalized = scaler.fit_transform(x.reshape(-1, x.shape[-1])).reshape(x.shape)\n",
    "\n",
    "# Define the attention-based CNN model\n",
    "def build_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Define the CNN layers\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = MaxPooling2D(pool_size=(1, 1))(x)  # Pooling with (1, 1) to avoid reducing dimensions too much\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D(pool_size=(1, 1))(x)  # Pooling with (1, 1)\n",
    "    \n",
    "    # Attention module\n",
    "    def attention_module(x):\n",
    "        # Channel attention\n",
    "        avg_pool = GlobalAveragePooling2D()(x)\n",
    "        max_pool = GlobalMaxPooling2D()(x)\n",
    "        avg_pool = Reshape((1, 1, x.shape[-1]))(avg_pool)\n",
    "        max_pool = Reshape((1, 1, x.shape[-1]))(max_pool)\n",
    "        concat = Concatenate(axis=-1)([avg_pool, max_pool])\n",
    "        concat = Conv2D(x.shape[-1], (1, 1), activation='sigmoid')(concat)\n",
    "        channel_att = Multiply()([x, concat])\n",
    "        \n",
    "        # Spatial attention\n",
    "        avg_pool = Lambda(lambda x: tf.reduce_mean(x, axis=-1, keepdims=True))(channel_att)\n",
    "        max_pool = Lambda(lambda x: tf.reduce_max(x, axis=-1, keepdims=True))(channel_att)\n",
    "        concat = Concatenate(axis=-1)([avg_pool, max_pool])\n",
    "        concat = Conv2D(1, (7, 7), activation='sigmoid', padding='same')(concat)\n",
    "        spatial_att = Multiply()([x, concat])\n",
    "        \n",
    "        return spatial_att\n",
    "    \n",
    "    x = attention_module(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    outputs = Dense(10, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Prepare the data for training\n",
    "y = np.random.randint(0, 10, size=(x_reshaped.shape[0],))  # Example labels, replace with actual labels\n",
    "\n",
    "# Split into training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(x_reshaped, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build and train the model\n",
    "input_shape = (x_reshaped.shape[1], x_reshaped.shape[2], x_reshaped.shape[3])\n",
    "model = build_model(input_shape)\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32)\n",
    "\n",
    "# Extract attention weights\n",
    "attention_layer = model.get_layer(index=3)  # Adjust index as needed\n",
    "attention_model = Model(inputs=model.input, outputs=attention_layer.output)\n",
    "attention_heatmaps = attention_model.predict(X_val)\n",
    "\n",
    "# Convert attention heatmaps to numpy for further analysis\n",
    "attention_scores = np.mean(attention_heatmaps, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "186855a6-96b0-4f3a-925b-86b7321cac16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harsha/.local/lib/python3.10/site-packages/sklearn/covariance/_robust_covariance.py:747: UserWarning: The covariance matrix associated to your dataset is not full rank\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of important bands: 40\n",
      "Indices of important bands: [  0   1   2   3   4   9  10  13  14  18  19  20  38  40  41  42  43  44\n",
      "  46  47  49  51  52  53  54  56  58  59  60  61  62  63  65  66  73 103\n",
      " 104 106 144 199]\n"
     ]
    }
   ],
   "source": [
    "attention_scores_flattened = attention_scores.reshape(-1, attention_scores.shape[-1])\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "attention_scores_flattened = scaler.fit_transform(attention_scores_flattened)\n",
    "\n",
    "# Debugging: Check if there are any NaNs or infinities in the data\n",
    "if np.any(np.isnan(attention_scores_flattened)) or np.any(np.isinf(attention_scores_flattened)):\n",
    "    raise ValueError(\"Data contains NaNs or infinities. Check the data preprocessing.\")\n",
    "\n",
    "# Initialize EllipticEnvelope with a larger support_fraction if needed\n",
    "ee = EllipticEnvelope(contamination=0.15, support_fraction=0.8)  # Adjust support_fraction as needed\n",
    "\n",
    "try:\n",
    "    # Fit the model\n",
    "    ee.fit(attention_scores_flattened)\n",
    "    \n",
    "    # Predict anomalies\n",
    "    important_bands = ee.predict(attention_scores_flattened)\n",
    "    \n",
    "    # Get indices where important_bands == -1\n",
    "    important_band_indices = np.where(important_bands == -1)[0]\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Number of important bands:\", len(important_band_indices))\n",
    "    print(\"Indices of important bands:\", important_band_indices)\n",
    "    \n",
    "except ValueError as e:\n",
    "    print(\"Error during fitting or prediction:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8b5372a7-a2f3-4706-8c71-8ae700a27408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of important bands: 36\n",
      "Important Bands:\n",
      "[-1 -1 -1 -1 -1  1  1  1  1 -1 -1  1  1 -1 -1  1  1 -1 -1 -1 -1 -1 -1 -1\n",
      "  1  1  1  1  1  1 -1  1  1  1  1  1  1  1 -1  1 -1  1  1  1 -1  1  1  1\n",
      "  1 -1  1  1  1  1  1  1  1  1 -1 -1 -1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1 -1 -1 -1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1\n",
      " -1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1\n",
      "  1  1  1 -1  1 -1 -1 -1]\n",
      "Indices of important bands: [  0   1   2   3   4   9  10  13  14  17  18  19  20  21  22  23  30  38\n",
      "  40  44  49  58  59  60 102 103 104 106 143 144 150 184 195 197 198 199]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Use Isolation Forest for anomaly detection\n",
    "iso_forest = IsolationForest(contamination=0.18)\n",
    "important_bands = iso_forest.fit_predict(attention_scores_flattened)\n",
    "\n",
    "print(\"Number of important bands:\", np.sum(important_bands == -1))  # -1 indicates anomalies\n",
    "print(\"Important Bands:\")\n",
    "print(important_bands)\n",
    "# Assuming `important_bands` contains the classification results from Isolation Forest\n",
    "# Get indices where important_bands == -1\n",
    "important_band_indices = np.where(important_bands == -1)[0]\n",
    "\n",
    "print(\"Indices of important bands:\", important_band_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "14ef5628-f6f2-4e10-83f0-5f449e21ac76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of important bands: 10\n",
      "Important Bands: [ 1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1\n",
      " -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1 -1  1  1  1  1  1  1 -1  1  1 -1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1 -1  1  1\n",
      "  1  1  1  1 -1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1]\n",
      "Indices of important bands: [  8  39  48 102 109 112 162 165 172 174]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# Apply LOF for anomaly detection\n",
    "lof = LocalOutlierFactor(contamination=0.05)\n",
    "important_bands = lof.fit_predict(attention_scores_flattened)\n",
    "\n",
    "# Print results\n",
    "num_important_bands = np.sum(important_bands == -1)\n",
    "print(\"Number of important bands:\", num_important_bands)\n",
    "print(\"Important Bands:\", important_bands)\n",
    "# Assuming `important_bands` contains the classification results from Isolation Forest\n",
    "# Get indices where important_bands == -1\n",
    "important_band_indices = np.where(important_bands == -1)[0]\n",
    "\n",
    "print(\"Indices of important bands:\", important_band_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fc8c21-b8f2-4b79-8655-18b04d071f39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
